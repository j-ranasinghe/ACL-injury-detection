{
  "cells": [
    {
      "metadata": {
        "id": "2816a1c7c8755091"
      },
      "cell_type": "markdown",
      "source": [
        "# **Detection of ACL injuries using MRNet model  - 2018**"
      ],
      "id": "2816a1c7c8755091"
    },
    {
      "metadata": {
        "id": "183ddb75e3f6ed83"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "id": "183ddb75e3f6ed83"
    },
    {
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-12-23T11:25:57.789888Z",
          "start_time": "2024-12-23T11:25:57.786465Z"
        },
        "id": "initial_id"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import shutil\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision.transforms import RandomRotation, RandomAffine, RandomHorizontalFlip, ToTensor, Compose\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from dataloader import MRDataset\n",
        "import model\n",
        "\n",
        "from sklearn import metrics\n"
      ],
      "id": "initial_id"
    },
    {
      "metadata": {
        "id": "33ea92557d8add7b"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ],
      "id": "33ea92557d8add7b"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-23T11:25:57.811178Z",
          "start_time": "2024-12-23T11:25:57.803911Z"
        },
        "id": "d18d35fa337bd9ff"
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, epoch, num_epochs, optimizer, writer, current_lr, log_every=100):\n",
        "    _ = model.train()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    y_preds = []\n",
        "    y_trues = []\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i, (image, label, weight) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "            weight = weight.cuda()\n",
        "\n",
        "        label = label[0]\n",
        "        weight = weight[0]\n",
        "\n",
        "        prediction = model.forward(image.float())\n",
        "        loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_value = loss.item()\n",
        "        losses.append(loss_value)\n",
        "\n",
        "        probas = torch.sigmoid(prediction)\n",
        "\n",
        "        y_trues.append(int(label[0][1]))\n",
        "        y_preds.append(probas[0][1].item())\n",
        "\n",
        "        correct_predictions = (torch.round(torch.Tensor(y_preds)) == torch.Tensor(y_trues)).sum().item()\n",
        "        accuracy = correct_predictions / len(y_trues)\n",
        "\n",
        "        try:\n",
        "            auc = metrics.roc_auc_score(y_trues, y_preds)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "\n",
        "        writer.add_scalar('Train/Loss', loss_value, epoch * len(train_loader) + i)\n",
        "        writer.add_scalar('Train/AUC', auc, epoch * len(train_loader) + i)\n",
        "        writer.add_scalar('Train/Accuracy', accuracy, epoch * len(train_loader) + i)\n",
        "        if (i % log_every == 0) & (i > 0):\n",
        "            print('''[Epoch: {0} / {1} |Single batch number : {2} / {3} ]| avg train loss {4} | train auc : {5} | train accuracy: {6} | lr : {7}'''.\n",
        "                  format(\n",
        "                      epoch + 1,\n",
        "                      num_epochs,\n",
        "                      i,\n",
        "                      len(train_loader),\n",
        "                      np.round(np.mean(losses), 4),\n",
        "                      np.round(auc, 4),\n",
        "                      np.round(accuracy, 4),\n",
        "                      current_lr\n",
        "                  )\n",
        "                  )\n",
        "\n",
        "    writer.add_scalar('Train/Accuracy_epoch', accuracy, epoch + i)\n",
        "    writer.add_scalar('Train/AUC_epoch', auc, epoch + i)\n",
        "\n",
        "    train_loss_epoch = np.round(np.mean(losses), 4)\n",
        "    train_auc_epoch = np.round(auc, 4)\n",
        "    return train_loss_epoch, train_auc_epoch, accuracy\n"
      ],
      "id": "d18d35fa337bd9ff",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "723d2b941a005951"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation Function"
      ],
      "id": "723d2b941a005951"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-23T11:25:57.824744Z",
          "start_time": "2024-12-23T11:25:57.818303Z"
        },
        "id": "dc77021e04a742ac"
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader, epoch, num_epochs, writer, current_lr, log_every=20):\n",
        "    _ = model.eval()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    y_trues = []\n",
        "    y_preds = []\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i, (image, label, weight) in enumerate(val_loader):\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "            weight = weight.cuda()\n",
        "\n",
        "        label = label[0]\n",
        "        weight = weight[0]\n",
        "\n",
        "        prediction = model.forward(image.float())\n",
        "        loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n",
        "\n",
        "        loss_value = loss.item()\n",
        "        losses.append(loss_value)\n",
        "\n",
        "        probas = torch.sigmoid(prediction)\n",
        "\n",
        "        y_trues.append(int(label[0][1]))\n",
        "        y_preds.append(probas[0][1].item())\n",
        "\n",
        "        correct_predictions = (torch.round(torch.Tensor(y_preds)) == torch.Tensor(y_trues)).sum().item()\n",
        "        accuracy = correct_predictions / len(y_trues)\n",
        "\n",
        "        try:\n",
        "            auc = metrics.roc_auc_score(y_trues, y_preds)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "\n",
        "        writer.add_scalar('Val/Loss', loss_value, epoch * len(val_loader) + i)\n",
        "        writer.add_scalar('Val/AUC', auc, epoch * len(val_loader) + i)\n",
        "        writer.add_scalar('Train/Accuracy', accuracy, epoch * len(val_loader) + i)\n",
        "        if (i % log_every == 0) & (i > 0):\n",
        "            print('''[Epoch: {0} / {1} |Single batch number : {2} / {3} ] | avg val loss {4} | val auc : {5} | val accuracy: {6} | lr : {7}'''.\n",
        "                  format(\n",
        "                      epoch + 1,\n",
        "                      num_epochs,\n",
        "                      i,\n",
        "                      len(val_loader),\n",
        "                      np.round(np.mean(losses), 4),\n",
        "                      np.round(auc, 4),\n",
        "                      np.round(accuracy, 4),\n",
        "                      current_lr\n",
        "                  )\n",
        "                  )\n",
        "\n",
        "    writer.add_scalar('Val/Accuracy_epoch', accuracy, epoch + i)\n",
        "    writer.add_scalar('Val/AUC_epoch', auc, epoch + i)\n",
        "\n",
        "    val_loss_epoch = np.round(np.mean(losses), 4)\n",
        "    val_auc_epoch = np.round(auc, 4)\n",
        "    return val_loss_epoch, val_auc_epoch, accuracy\n"
      ],
      "id": "dc77021e04a742ac",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-23T11:25:57.845358Z",
          "start_time": "2024-12-23T11:25:57.842094Z"
        },
        "id": "a3ef9d826bb1021"
      },
      "cell_type": "code",
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n"
      ],
      "id": "a3ef9d826bb1021",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eab8e296d9a9d89a"
      },
      "cell_type": "markdown",
      "source": [
        "## Main Run"
      ],
      "id": "eab8e296d9a9d89a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-23T11:25:57.889778Z",
          "start_time": "2024-12-23T11:25:57.880719Z"
        },
        "id": "d79f6383d616c4d3"
      },
      "cell_type": "code",
      "source": [
        "def run(args):\n",
        "    log_root_folder = \"./logs/{0}/{1}/\".format(args.task, args.plane)\n",
        "    if args.flush_history == 1:\n",
        "        objects = os.listdir(log_root_folder)\n",
        "        for f in objects:\n",
        "            if os.path.isdir(log_root_folder + f):\n",
        "                shutil.rmtree(log_root_folder + f)\n",
        "\n",
        "    now = datetime.now()\n",
        "    logdir = log_root_folder + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "    writer = SummaryWriter(logdir)\n",
        "\n",
        "    augmentor = Compose([\n",
        "        transforms.Lambda(lambda x: torch.Tensor(x)),\n",
        "        RandomRotation(25),\n",
        "        RandomAffine(degrees=0, translate=(0.11, 0.11)),\n",
        "        RandomHorizontalFlip(p=0.5)        ,\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1, 1).permute(1, 0, 2, 3)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = MRDataset('./data/', args.task,\n",
        "                              args.plane, transform=augmentor, train=True)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)\n",
        "\n",
        "    validation_dataset = MRDataset(\n",
        "        './data/', args.task, args.plane, train=False)\n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        validation_dataset, batch_size=1, shuffle=-True, num_workers=0, drop_last=False)\n",
        "\n",
        "    mrnet = model.MRNet()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        mrnet = mrnet.cuda()\n",
        "\n",
        "    optimizer = optim.Adam(mrnet.parameters(), lr=args.lr, weight_decay=0.1)\n",
        "\n",
        "    if args.lr_scheduler == \"plateau\":\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=3, factor=.3, threshold=1e-4, verbose=True)\n",
        "    elif args.lr_scheduler == \"step\":\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer, step_size=3, gamma=args.gamma)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_auc = float(0)\n",
        "\n",
        "    num_epochs = args.epochs\n",
        "    iteration_change_loss = 0\n",
        "    patience = args.patience\n",
        "    log_every = args.log_every\n",
        "\n",
        "    t_start_training = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        current_lr = get_lr(optimizer)\n",
        "\n",
        "        t_start = time.time()\n",
        "\n",
        "        train_loss, train_auc, accuracy = train_model(\n",
        "            mrnet, train_loader, epoch, num_epochs, optimizer, writer, current_lr, log_every)\n",
        "        val_loss, val_auc, accuracy = evaluate_model(\n",
        "            mrnet, validation_loader, epoch, num_epochs, writer, current_lr)\n",
        "\n",
        "        if args.lr_scheduler == 'plateau':\n",
        "            scheduler.step(val_loss)\n",
        "        elif args.lr_scheduler == 'step':\n",
        "            scheduler.step()\n",
        "\n",
        "        t_end = time.time()\n",
        "        delta = t_end - t_start\n",
        "\n",
        "        print(\"train loss : {0} | train auc {1} | val loss {2} | val auc {3} | elapsed time {4} s\".format(\n",
        "            train_loss, train_auc, val_loss, val_auc, delta))\n",
        "\n",
        "        iteration_change_loss += 1\n",
        "        print('-' * 30)\n",
        "\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            if bool(args.save_model):\n",
        "                file_name = f'model_{args.prefix_name}_{args.task}_{args.plane}_val_auc_{val_auc:0.4f}_train_auc_{train_auc:0.4f}_epoch_{epoch+1}.pth'\n",
        "                for f in os.listdir('./models/'):\n",
        "                    if (args.task in f) and (args.plane in f) and (args.prefix_name in f):\n",
        "                        os.remove(f'./models/{f}')\n",
        "                torch.save(mrnet, f'./models/{file_name}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            iteration_change_loss = 0\n",
        "\n",
        "        if iteration_change_loss == patience:\n",
        "            print('Early stopping after {0} iterations without the decrease of the val loss'.\n",
        "                  format(iteration_change_loss))\n",
        "            break\n",
        "\n",
        "    t_end_training = time.time()\n",
        "    print(f'training took {t_end_training - t_start_training} s')"
      ],
      "id": "d79f6383d616c4d3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "9b5774197d0b7398"
      },
      "cell_type": "markdown",
      "source": [
        "## Arguments"
      ],
      "id": "9b5774197d0b7398"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-23T13:03:31.520225Z",
          "start_time": "2024-12-23T11:26:44.948200Z"
        },
        "id": "e47f3f8e78bf630d",
        "outputId": "e9757387-fb1e-4db5-df93-5c60cad3e160"
      },
      "cell_type": "code",
      "source": [
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-t', '--task', type=str, required=True,\n",
        "                        choices=['abnormal', 'acl', 'meniscus'])\n",
        "    parser.add_argument('-p', '--plane', type=str, required=True,\n",
        "                        choices=['sagittal', 'coronal', 'axial'])\n",
        "    parser.add_argument('--prefix_name', type=str, required=True)\n",
        "    parser.add_argument('--augment', type=int, choices=[0, 1], default=1)\n",
        "    parser.add_argument('--lr_scheduler', type=str,\n",
        "                        default='plateau', choices=['plateau', 'step'])\n",
        "    parser.add_argument('--gamma', type=float, default=0.5)\n",
        "    parser.add_argument('--epochs', type=int, default=10)\n",
        "    parser.add_argument('--lr', type=float, default=1e-5)\n",
        "    parser.add_argument('--flush_history', type=int, choices=[0, 1], default=0)\n",
        "    parser.add_argument('--save_model', type=int, choices=[0, 1], default=1)\n",
        "    parser.add_argument('--patience', type=int, default=5)\n",
        "    parser.add_argument('--log_every', type=int, default=100)\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.argv = [\n",
        "    \"ipykernel_launcher.py\",\n",
        "    \"-t\", \"acl\",\n",
        "    \"-p\", \"sagittal\",\n",
        "    \"--prefix_name\", \"notebook_test\",\n",
        "    \"--augment\", \"1\"\n",
        "]\n",
        "\n",
        "args = parse_arguments()\n",
        "run(args)\n"
      ],
      "id": "e47f3f8e78bf630d",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kasun\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\kasun\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "C:\\Users\\kasun\\miniconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 10 |Single batch number : 100 / 1130 ]| avg train loss 6.0402 | train auc : 0.4524 | train accuracy: 0.6535 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 200 / 1130 ]| avg train loss 4.4876 | train auc : 0.4308 | train accuracy: 0.6816 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 300 / 1130 ]| avg train loss 3.8226 | train auc : 0.4461 | train accuracy: 0.6877 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 400 / 1130 ]| avg train loss 3.3305 | train auc : 0.4875 | train accuracy: 0.7132 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 500 / 1130 ]| avg train loss 3.027 | train auc : 0.5001 | train accuracy: 0.7285 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 600 / 1130 ]| avg train loss 2.8571 | train auc : 0.507 | train accuracy: 0.7255 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 700 / 1130 ]| avg train loss 2.6776 | train auc : 0.5237 | train accuracy: 0.7332 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 800 / 1130 ]| avg train loss 2.5063 | train auc : 0.5434 | train accuracy: 0.7441 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 900 / 1130 ]| avg train loss 2.4425 | train auc : 0.5431 | train accuracy: 0.7414 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 2.3786 | train auc : 0.5478 | train accuracy: 0.7433 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 2.3084 | train auc : 0.5572 | train accuracy: 0.7421 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.7525 | val auc : 0.6889 | val accuracy: 0.8095 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 40 / 120 ] | avg val loss 1.1335 | val auc : 0.6324 | val accuracy: 0.6585 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 60 / 120 ] | avg val loss 1.0266 | val auc : 0.7044 | val accuracy: 0.6885 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 80 / 120 ] | avg val loss 1.0176 | val auc : 0.7058 | val accuracy: 0.679 | lr : 1e-05\n",
            "[Epoch: 1 / 10 |Single batch number : 100 / 120 ] | avg val loss 1.0765 | val auc : 0.6828 | val accuracy: 0.6436 | lr : 1e-05\n",
            "train loss : 2.2772 | train auc 0.5576 | val loss 1.1676 | val auc 0.6661 | elapsed time 577.9625163078308 s\n",
            "------------------------------\n",
            "[Epoch: 2 / 10 |Single batch number : 100 / 1130 ]| avg train loss 1.6716 | train auc : 0.625 | train accuracy: 0.7624 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 200 / 1130 ]| avg train loss 1.435 | train auc : 0.6887 | train accuracy: 0.8209 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 300 / 1130 ]| avg train loss 1.2817 | train auc : 0.6805 | train accuracy: 0.8405 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 400 / 1130 ]| avg train loss 1.4172 | train auc : 0.6603 | train accuracy: 0.813 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.4452 | train auc : 0.6453 | train accuracy: 0.8104 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.4311 | train auc : 0.6445 | train accuracy: 0.8087 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.4624 | train auc : 0.6385 | train accuracy: 0.796 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.424 | train auc : 0.6428 | train accuracy: 0.804 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.4498 | train auc : 0.635 | train accuracy: 0.798 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.4154 | train auc : 0.6386 | train accuracy: 0.8032 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.4403 | train auc : 0.6349 | train accuracy: 0.7965 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.7348 | val auc : 0.8056 | val accuracy: 0.8095 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.7947 | val auc : 0.686 | val accuracy: 0.7073 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.812 | val auc : 0.757 | val accuracy: 0.6885 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.7775 | val auc : 0.7432 | val accuracy: 0.7037 | lr : 1e-05\n",
            "[Epoch: 2 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.7871 | val auc : 0.7261 | val accuracy: 0.6832 | lr : 1e-05\n",
            "train loss : 1.4316 | train auc 0.6369 | val loss 0.7643 | val auc 0.736 | elapsed time 577.0915105342865 s\n",
            "------------------------------\n",
            "[Epoch: 3 / 10 |Single batch number : 100 / 1130 ]| avg train loss 1.2424 | train auc : 0.6867 | train accuracy: 0.8317 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 200 / 1130 ]| avg train loss 1.3508 | train auc : 0.6801 | train accuracy: 0.8109 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 300 / 1130 ]| avg train loss 1.315 | train auc : 0.6963 | train accuracy: 0.814 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 400 / 1130 ]| avg train loss 1.2449 | train auc : 0.7283 | train accuracy: 0.8204 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.2696 | train auc : 0.703 | train accuracy: 0.8224 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.2819 | train auc : 0.7056 | train accuracy: 0.8236 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.2514 | train auc : 0.7033 | train accuracy: 0.8288 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.2601 | train auc : 0.6947 | train accuracy: 0.829 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.2734 | train auc : 0.6817 | train accuracy: 0.8246 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.2771 | train auc : 0.6809 | train accuracy: 0.8232 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.3055 | train auc : 0.6771 | train accuracy: 0.8156 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.7676 | val auc : 0.7727 | val accuracy: 0.7619 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.6648 | val auc : 0.7593 | val accuracy: 0.7805 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.6789 | val auc : 0.8028 | val accuracy: 0.7541 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.6446 | val auc : 0.8061 | val accuracy: 0.7531 | lr : 1e-05\n",
            "[Epoch: 3 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.6791 | val auc : 0.7783 | val accuracy: 0.7228 | lr : 1e-05\n",
            "train loss : 1.3052 | train auc 0.6775 | val loss 0.7004 | val auc 0.7753 | elapsed time 584.5996823310852 s\n",
            "------------------------------\n",
            "[Epoch: 4 / 10 |Single batch number : 100 / 1130 ]| avg train loss 1.4246 | train auc : 0.6401 | train accuracy: 0.7525 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 200 / 1130 ]| avg train loss 1.4458 | train auc : 0.6358 | train accuracy: 0.7711 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 300 / 1130 ]| avg train loss 1.4273 | train auc : 0.6739 | train accuracy: 0.7807 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 400 / 1130 ]| avg train loss 1.3835 | train auc : 0.6665 | train accuracy: 0.783 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.374 | train auc : 0.6497 | train accuracy: 0.7904 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.333 | train auc : 0.6567 | train accuracy: 0.802 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.2838 | train auc : 0.6704 | train accuracy: 0.8103 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.2602 | train auc : 0.6904 | train accuracy: 0.8102 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.2867 | train auc : 0.6885 | train accuracy: 0.8024 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.2713 | train auc : 0.6866 | train accuracy: 0.8072 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.2717 | train auc : 0.6752 | train accuracy: 0.8102 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.884 | val auc : 0.8269 | val accuracy: 0.5238 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.8082 | val auc : 0.8636 | val accuracy: 0.5366 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.7868 | val auc : 0.7931 | val accuracy: 0.5902 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.7562 | val auc : 0.8048 | val accuracy: 0.6173 | lr : 1e-05\n",
            "[Epoch: 4 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.7725 | val auc : 0.7644 | val accuracy: 0.604 | lr : 1e-05\n",
            "train loss : 1.2829 | train auc 0.6731 | val loss 0.7596 | val auc 0.7525 | elapsed time 581.6926069259644 s\n",
            "------------------------------\n",
            "[Epoch: 5 / 10 |Single batch number : 100 / 1130 ]| avg train loss 1.4863 | train auc : 0.5619 | train accuracy: 0.7921 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 200 / 1130 ]| avg train loss 1.2906 | train auc : 0.6388 | train accuracy: 0.8209 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 300 / 1130 ]| avg train loss 1.216 | train auc : 0.6732 | train accuracy: 0.8272 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 400 / 1130 ]| avg train loss 1.1923 | train auc : 0.6884 | train accuracy: 0.8229 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.156 | train auc : 0.6936 | train accuracy: 0.8323 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.1844 | train auc : 0.69 | train accuracy: 0.8303 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.2076 | train auc : 0.6992 | train accuracy: 0.8245 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.2024 | train auc : 0.719 | train accuracy: 0.8265 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.179 | train auc : 0.7218 | train accuracy: 0.8313 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.181 | train auc : 0.7221 | train accuracy: 0.8302 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.1862 | train auc : 0.7265 | train accuracy: 0.8265 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.5167 | val auc : 0.875 | val accuracy: 0.7619 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.656 | val auc : 0.819 | val accuracy: 0.7317 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.6233 | val auc : 0.8387 | val accuracy: 0.7541 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.6051 | val auc : 0.8458 | val accuracy: 0.7654 | lr : 1e-05\n",
            "[Epoch: 5 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.5992 | val auc : 0.8481 | val accuracy: 0.7723 | lr : 1e-05\n",
            "train loss : 1.1887 | train auc 0.731 | val loss 0.6164 | val auc 0.8395 | elapsed time 580.6401746273041 s\n",
            "------------------------------\n",
            "[Epoch: 6 / 10 |Single batch number : 100 / 1130 ]| avg train loss 1.2482 | train auc : 0.708 | train accuracy: 0.8119 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 200 / 1130 ]| avg train loss 1.3163 | train auc : 0.6801 | train accuracy: 0.7761 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 300 / 1130 ]| avg train loss 1.3 | train auc : 0.6726 | train accuracy: 0.7907 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 400 / 1130 ]| avg train loss 1.2861 | train auc : 0.6941 | train accuracy: 0.8005 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.2752 | train auc : 0.7032 | train accuracy: 0.8044 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.2718 | train auc : 0.6923 | train accuracy: 0.8053 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.2644 | train auc : 0.6798 | train accuracy: 0.8088 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.2372 | train auc : 0.6887 | train accuracy: 0.8115 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.2181 | train auc : 0.7029 | train accuracy: 0.8124 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.2231 | train auc : 0.6958 | train accuracy: 0.8132 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.2324 | train auc : 0.6967 | train accuracy: 0.812 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.9154 | val auc : 0.7593 | val accuracy: 0.5714 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.737 | val auc : 0.6852 | val accuracy: 0.6585 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.7555 | val auc : 0.7254 | val accuracy: 0.6393 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.8434 | val auc : 0.7564 | val accuracy: 0.6049 | lr : 1e-05\n",
            "[Epoch: 6 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.774 | val auc : 0.781 | val accuracy: 0.6436 | lr : 1e-05\n",
            "train loss : 1.2292 | train auc 0.6942 | val loss 0.7306 | val auc 0.81 | elapsed time 582.9713683128357 s\n",
            "------------------------------\n",
            "[Epoch: 7 / 10 |Single batch number : 100 / 1130 ]| avg train loss 1.2415 | train auc : 0.6778 | train accuracy: 0.8515 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 200 / 1130 ]| avg train loss 1.2439 | train auc : 0.6852 | train accuracy: 0.8308 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 300 / 1130 ]| avg train loss 1.2176 | train auc : 0.7209 | train accuracy: 0.8206 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 400 / 1130 ]| avg train loss 1.1798 | train auc : 0.7475 | train accuracy: 0.818 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.166 | train auc : 0.7399 | train accuracy: 0.8244 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.1479 | train auc : 0.7372 | train accuracy: 0.827 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.0951 | train auc : 0.7589 | train accuracy: 0.8359 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.1039 | train auc : 0.7414 | train accuracy: 0.8365 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.1218 | train auc : 0.7312 | train accuracy: 0.8335 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.1504 | train auc : 0.7236 | train accuracy: 0.8282 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.1623 | train auc : 0.729 | train accuracy: 0.8247 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.5638 | val auc : 0.8056 | val accuracy: 0.7619 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.5347 | val auc : 0.8929 | val accuracy: 0.7561 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.613 | val auc : 0.8484 | val accuracy: 0.7213 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.5661 | val auc : 0.859 | val accuracy: 0.7654 | lr : 1e-05\n",
            "[Epoch: 7 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.5758 | val auc : 0.8617 | val accuracy: 0.7822 | lr : 1e-05\n",
            "train loss : 1.1562 | train auc 0.7331 | val loss 0.5722 | val auc 0.853 | elapsed time 579.8643369674683 s\n",
            "------------------------------\n",
            "[Epoch: 8 / 10 |Single batch number : 100 / 1130 ]| avg train loss 0.6415 | train auc : 0.8563 | train accuracy: 0.9109 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 200 / 1130 ]| avg train loss 1.0109 | train auc : 0.787 | train accuracy: 0.8607 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 300 / 1130 ]| avg train loss 1.0478 | train auc : 0.7971 | train accuracy: 0.8472 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 400 / 1130 ]| avg train loss 1.0598 | train auc : 0.801 | train accuracy: 0.8504 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.042 | train auc : 0.7914 | train accuracy: 0.8503 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.0649 | train auc : 0.7789 | train accuracy: 0.8436 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.0551 | train auc : 0.7889 | train accuracy: 0.8402 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.0731 | train auc : 0.7811 | train accuracy: 0.834 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.1108 | train auc : 0.7735 | train accuracy: 0.8269 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.1217 | train auc : 0.768 | train accuracy: 0.8232 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.1284 | train auc : 0.7684 | train accuracy: 0.8211 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.566 | val auc : 0.9231 | val accuracy: 0.7619 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.6259 | val auc : 0.8524 | val accuracy: 0.7805 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.5691 | val auc : 0.866 | val accuracy: 0.7869 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.569 | val auc : 0.8612 | val accuracy: 0.7654 | lr : 1e-05\n",
            "[Epoch: 8 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.6053 | val auc : 0.8526 | val accuracy: 0.7327 | lr : 1e-05\n",
            "train loss : 1.1358 | train auc 0.7637 | val loss 0.6075 | val auc 0.8423 | elapsed time 579.8131773471832 s\n",
            "------------------------------\n",
            "[Epoch: 9 / 10 |Single batch number : 100 / 1130 ]| avg train loss 1.0941 | train auc : 0.8056 | train accuracy: 0.8317 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 200 / 1130 ]| avg train loss 1.0163 | train auc : 0.7711 | train accuracy: 0.8507 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 300 / 1130 ]| avg train loss 1.0071 | train auc : 0.7782 | train accuracy: 0.8439 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 400 / 1130 ]| avg train loss 1.0355 | train auc : 0.7634 | train accuracy: 0.8429 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.082 | train auc : 0.7364 | train accuracy: 0.8403 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.0675 | train auc : 0.7693 | train accuracy: 0.8369 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.1164 | train auc : 0.7585 | train accuracy: 0.8245 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.1177 | train auc : 0.765 | train accuracy: 0.8202 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.1304 | train auc : 0.7656 | train accuracy: 0.8202 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.1344 | train auc : 0.7591 | train accuracy: 0.8192 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.1184 | train auc : 0.7596 | train accuracy: 0.8229 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 20 / 120 ] | avg val loss 0.9584 | val auc : 0.7727 | val accuracy: 0.5714 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.8574 | val auc : 0.7656 | val accuracy: 0.6341 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.7492 | val auc : 0.8214 | val accuracy: 0.7049 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.8002 | val auc : 0.8223 | val accuracy: 0.6667 | lr : 1e-05\n",
            "[Epoch: 9 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.7371 | val auc : 0.8209 | val accuracy: 0.703 | lr : 1e-05\n",
            "train loss : 1.1352 | train auc 0.7533 | val loss 0.752 | val auc 0.8033 | elapsed time 578.5912277698517 s\n",
            "------------------------------\n",
            "[Epoch: 10 / 10 |Single batch number : 100 / 1130 ]| avg train loss 1.0071 | train auc : 0.8479 | train accuracy: 0.8317 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 200 / 1130 ]| avg train loss 0.9511 | train auc : 0.8324 | train accuracy: 0.8607 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 300 / 1130 ]| avg train loss 0.9329 | train auc : 0.8304 | train accuracy: 0.8671 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 400 / 1130 ]| avg train loss 0.9577 | train auc : 0.7967 | train accuracy: 0.8653 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 500 / 1130 ]| avg train loss 1.0209 | train auc : 0.7875 | train accuracy: 0.8543 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 600 / 1130 ]| avg train loss 1.0243 | train auc : 0.7987 | train accuracy: 0.8502 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 700 / 1130 ]| avg train loss 1.0518 | train auc : 0.7839 | train accuracy: 0.8474 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 800 / 1130 ]| avg train loss 1.065 | train auc : 0.7863 | train accuracy: 0.8414 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 900 / 1130 ]| avg train loss 1.0902 | train auc : 0.7729 | train accuracy: 0.8402 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 1000 / 1130 ]| avg train loss 1.1281 | train auc : 0.7555 | train accuracy: 0.8352 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 1100 / 1130 ]| avg train loss 1.1287 | train auc : 0.7498 | train accuracy: 0.8329 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 20 / 120 ] | avg val loss 1.1195 | val auc : 0.75 | val accuracy: 0.5238 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 40 / 120 ] | avg val loss 0.9399 | val auc : 0.8134 | val accuracy: 0.5854 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 60 / 120 ] | avg val loss 0.8231 | val auc : 0.8203 | val accuracy: 0.6557 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 80 / 120 ] | avg val loss 0.7776 | val auc : 0.8256 | val accuracy: 0.679 | lr : 1e-05\n",
            "[Epoch: 10 / 10 |Single batch number : 100 / 120 ] | avg val loss 0.8178 | val auc : 0.8141 | val accuracy: 0.6634 | lr : 1e-05\n",
            "train loss : 1.1278 | train auc 0.7509 | val loss 0.7676 | val auc 0.8401 | elapsed time 579.211413860321 s\n",
            "------------------------------\n",
            "training took 5805.912374258041 s\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4439f9803f3822d9"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Results Summary\n",
        "\n",
        "#### Experiment: `Sagittal`\n",
        "\n",
        "| Metric           | Train Loss | Train AUC | Train Accuracy | Val Loss | Val AUC | Val Accuracy |\n",
        "|-------------------|------------|-----------|----------------|----------|---------|--------------|\n",
        "| Average  | 1.1278     | 0.7509    | 0.8329         | 0.7676   | 0.8401  | 0.6634       |"
      ],
      "id": "4439f9803f3822d9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}